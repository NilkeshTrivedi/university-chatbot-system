
AdmissAI â€” Project Documentation
# AdmissAI ğŸ“

Repository: github.com/NilkeshTrivedi/university-chatbot-system
Built for: GenAI4GenZ program by HPE
Stack: Python Â· Flask Â· Groq Â· Scaledown Â· Vanilla JS

**College Admissions Assistant Â· Built for GenAI4GenZ by HPE**

`Python` Â· `Flask` Â· `Groq` Â· `Vanilla JS` Â· `Scaledown`

---

## What Is It?

AdmissAI lets you pick a university program and chat with an AI that actually knows what that specific school is looking for â€” real GPA ranges, SAT scores, essay prompts, deadlines, and tips. Not guesses from training data. The actual requirements, loaded into every response.

The idea came from a simple frustration: most admissions advice online is too generic to act on. "Write a strong essay" is not useful when you need to know whether a 3.8 GPA is competitive for Caltech Physics, or what MIT specifically looks for in a Why MIT essay. Grounding the AI in real program data fixes that.

There's also a checklist for each program â€” every task you need to complete (essays, recs, tests, fee, interviews) organized by category and saved in your browser. And a program explorer where you can browse all eight schools and open a full detail view.

---

## ğŸ« Supported Programs

| Program ID | University | Program |
|---|---|---|
| `mit-cs` | MIT | Computer Science (B.S.) |
| `stanford-cs` | Stanford | Computer Science (B.S.) |
| `harvard-premed` | Harvard | Pre-Medicine / Biological Sciences |
| `wharton-business` | UPenn | Wharton Business Economics & Management |
| `cmu-cs` | Carnegie Mellon | Computer Science (B.S.) |
| `oxford-law` | Oxford | Law (BA/LLB) |
| `columbia-econ` | Columbia | Economics (B.A.) |
| `caltech-physics` | Caltech | Physics (B.S.) |

---

## âš™ï¸ Architecture

### How a chat message actually works

```
Your message
     â†“
POST /api/chat  (+ optional program_id)
     â†“
Fetch program requirements from admissions_data.py
     â†“
compression.py  â†’  Groq-powered (primary) or rule-based (fallback)
     â†“
Compressed context appended to system prompt
     â†“
Last 10 conversation turns added as messages array
     â†“
Groq  â†’  Llama 3.3 70B
     â†“
Reply + compression stats returned to frontend
```

Without a program selected, it's a direct chat â€” the AI answers from general training knowledge, which is fine for broad questions like how financial aid works or what a college essay structure should look like.

### Context Injection ğŸ’‰

The AI doesn't know program requirements by default â€” and even if it did, those numbers might be a year out of date. Instead, the actual requirements are fetched at request time and injected into the system prompt. The model answers from that data.

After compression, the injected block looks roughly like:

```
=== MIT Computer Science (B.S.) ===
Acceptance: 3.9% | GPA: 4.0+ | SAT: 1540-1590 | Deadline: Jan 1
Essays: 650w personal + 5 short activities + 250w Why MIT
Recs: 2 teacher (math/sci) + 1 counselor
Tips: Emphasise original research, open-source work...
Mistakes: Generic essays, weak Why MIT response...
```

The system prompt also tells the model not to fabricate statistics. Without that instruction, a language model will sometimes invent a plausible-sounding number. With it, it sticks to what's actually in the context.

### Compression ğŸ—œï¸

Program requirements run to about 300â€“500 tokens per message. Two tiers handle this:

| Tier | Method | Result |
|---|---|---|
| Primary | Groq asked to compress to ~50% while keeping every number, deadline, and requirement | AI-quality compression |
| Fallback | Rule-based abbreviation (`recommendations` â†’ `recs.`, `personal statement` â†’ `PS`, etc.) | Nothing breaks for the user |

Compression stats (tokens saved, ratio, provider) are returned with every response and shown in the UI. This was a late addition â€” it made a more noticeable difference than expected. Tighter context means the model focuses on the relevant parts rather than parsing filler phrasing.

### Conversation History ğŸ—£ï¸

The last 10 message turns are included in every API call. This means follow-ups work correctly â€” "What GPA do I need?" then "What about test scores?" without repeating yourself. History beyond 10 turns is dropped so token usage stays predictable.

---

## ğŸ“ File Breakdown

```
admissai/
â”œâ”€â”€ app.py                    Flask entry point
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ chat.py               POST /api/chat
â”‚   â”œâ”€â”€ programs.py           GET /api/programs, /api/programs/<id>
â”‚   â”œâ”€â”€ checklist.py          GET /api/checklist/<id>
â”‚   â””â”€â”€ compress.py           POST /api/compress
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ai_service.py         Groq call + prompt builder + error handling
â”‚   â”œâ”€â”€ admissions_data.py    All program data as Python dicts
â”‚   â””â”€â”€ compression.py        Groq compression with rule-based fallback
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            Single-page app â€” all four pages in one file
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/main.css          Dark theme, ~1000 lines, responsive
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ app.js            SPA router, global state, API helper, toasts
â”‚       â”œâ”€â”€ programs.js       Program grid, search, filter, detail drawer
â”‚       â”œâ”€â”€ chat.js           Chat UI, message rendering, markdown parser
â”‚       â””â”€â”€ checklist.js      Task toggle, localStorage, progress ring
â”œâ”€â”€ .env.example
â””â”€â”€ requirements.txt
```

### Backend files

**`app.py`** â€” Flask application factory. Creates the app, registers four blueprints, configures CORS. GET `/` returns index.html.

**`routes/chat.py`** â€” Handles POST `/api/chat`. Reads message, history, and program_id from the request body. Fetches requirements if a program is given, compresses them, builds the system prompt, calls ai_service. Returns the reply and compression_stats.

**`routes/programs.py`** â€” GET `/api/programs` returns the full list with optional `?q=` search and `?category=` filter. GET `/api/programs/<id>` returns one program in full. Both just read from admissions_data.py â€” no database.

**`services/ai_service.py`** â€” Builds the system prompt, appends compressed context, combines with conversation history, and sends the full payload to Groq over HTTP using the `requests` library (no SDK). Returns the reply text or a specific readable error for each failure case:

| Error | Message shown |
|---|---|
| `GROQ_API_KEY` not set | Setup instructions |
| Key is still a placeholder | Explanation with example |
| 401 Unauthorized | Verify key and restart |
| 429 Rate limited | Wait and try again |
| Connection error | Check internet |
| Timeout | Try again |
| Anything else | The actual exception message |

Specific errors matter. The first version returned a generic "something went wrong" on any failure â€” impossible to tell whether the key was missing, wrong, or rate-limited.

**`services/admissions_data.py`** â€” All program data in one place. Single source of truth for the explorer, AI context, and checklists.

**`services/compression.py`** â€” Tries Groq-powered compression first. Falls back to rule-based abbreviation. Returns compressed text and a stats object: `original_tokens`, `compressed_tokens`, `tokens_saved`, `compression_ratio`, `provider`.

### Frontend files

**`index.html`** â€” One file. All four pages (Home, Programs, Chat, Checklist) exist inside it. JavaScript swaps which one is visible. No build step, no templating engine.

**`app.js`** â€” SPA router and shared utilities. `App.navigate(page)` hides all pages, shows the target one, and calls that module's `init()`. `App.state` holds shared data. `App.api()` wraps fetch with JSON parsing and error throwing. `App.toast()` shows temporary notifications.

**`chat.js`** â€” Loads programs into the sidebar, tracks the selected program, sends messages, renders responses. Markdown in AI replies (bold, lists, code, headings) is converted by a line-by-line processor rather than regex, which was producing broken HTML on multi-line lists.

**`checklist.js`** â€” Fetches the checklist, renders tasks grouped by category, handles check/uncheck, saves to localStorage. The progress ring, percentage, done count, and subtitle text all update through a single `updateProgress()` function. Earlier they updated separately, which caused a decrement bug. localStorage key: `checklist_{program_id}`.

---

## ğŸ”Œ API Reference

| Method | Endpoint | Description |
|---|---|---|
| GET | `/api/health` | Returns `{"status": "ok"}` |
| GET | `/api/programs` | List all programs (`?q=` and `?category=`) |
| GET | `/api/programs/<id>` | Full detail for one program |
| POST | `/api/chat` | Send a message, get a reply |
| GET | `/api/checklist/<id>` | Checklist tasks grouped by category |
| POST | `/api/compress` | Compress any text string |

### POST /api/chat

**Request:**
```json
{
  "message": "What GPA do I need for MIT?",
  "history": [
    { "role": "user", "content": "..." },
    { "role": "assistant", "content": "..." }
  ],
  "program_id": "mit-cs"
}
```

**Response:**
```json
{
  "reply": "MIT requires a 4.0+ unweighted GPA...",
  "model_used": "groq/llama-3.3-70b-versatile",
  "compression_stats": {
    "original_tokens": 320,
    "compressed_tokens": 176,
    "compression_ratio": 0.45,
    "tokens_saved": 144,
    "provider": "rule-based"
  }
}
```

---

## ğŸš€ Setup

**Requirements:** Python 3.8+, a Groq API key from [console.groq.com](https://console.groq.com) (free tier works fine), internet connection.

```bash
git clone https://github.com/your-username/admissai.git
cd admissai
pip install flask flask-cors python-dotenv requests
```

Copy `.env.example` to `.env` and fill in your key:

```
GROQ_API_KEY=gsk_your_actual_key_here
FLASK_DEBUG=1
PORT=5000
```

```bash
python app.py
```

Open `http://localhost:5000`, go to AI Chat, and send a message to verify Groq is connected.

### Environment Variables

| Variable | Required | Default | Purpose |
|---|---|---|---|
| `GROQ_API_KEY` | âœ… Yes | â€” | From console.groq.com |
| `FLASK_DEBUG` | No | 0 | 1 for dev (auto-reload) |
| `PORT` | No | 5000 | Server port |
| `SCALEDOWN_API_KEY` | No | â€” | Reserved for future use |

### Dependencies

| Package | Version | Purpose |
|---|---|---|
| `flask` | 3.0.0 | Web framework |
| `flask-cors` | 4.0.0 | CORS headers |
| `python-dotenv` | 1.0.0 | Loads `.env` at startup |
| `requests` | 2.31.0 | HTTP client for Groq |

No Groq SDK â€” it caused installation issues in certain environments. Calling the API directly over HTTP worked everywhere and is simpler to debug.

---

## ğŸ› Problems I Ran Into

**Gemini integration silently failing** â€” The `google-generativeai` package was in requirements.txt but not actually installed. The code failed at import time and fell through to a hardcoded fallback response. Every single AI reply was that same fallback string with no visible error. Switched entirely to Groq and dropped the SDK approach â€” direct HTTP with `requests` removed the silent failure mode.

**Model decommissioned** â€” After getting Groq working, the model in the code was `llama3-8b-8192`. That model was decommissioned. Requests started returning HTTP 400. Updated to `llama-3.3-70b-versatile`.

**`App.addTokensSaved` is not a function** â€” When the frontend was rewritten to add the home page, `addTokensSaved()` was removed from the App module's public API. `chat.js` still called it on every program-context response. General chat worked fine because `compression_stats` is only returned when a `program_id` is sent â€” so the bug only showed up in one specific path, crashed the entire response render, and was annoying to track down. Added the function back.

**Checklist subtitle not updating** â€” Checking a task incremented the progress ring and the X/11 counter. The subtitle ("MIT CS Â· 5/11 tasks completed") was only set during initial render and never touched again. Fixed by extracting `updateProgress()` as a single function that updates the ring, percentage, counter, and subtitle all at once. Two separate code paths updating the same UI was always going to cause drift.

**Markdown lists rendering broken HTML** â€” AI responses often contain bullet lists. The first approach used a regex pass over the full text and would double-wrap `<ul>` tags on multi-line lists. Replaced with a line-by-line processor that tracks whether a list is currently open and closes it properly before switching to a new block type.

**Checklist refetching on every tab switch** â€” Every time you clicked the Checklist tab, it rebuilt the dropdown and refetched the program list. If a program was already loaded, you could see it flash. Fixed with a guard â€” only rebuild the dropdown if options haven't been added yet, only refetch checklist data if the selected program actually changed.

**Strikethrough on completed tasks** â€” Not a functional bug. Checking a task applied `text-decoration: line-through` which made longer task descriptions hard to read. The green checkbox already confirms completion. Removed the strikethrough, kept only the colour fade.

---

## âš ï¸ Limitations

- Program data is hardcoded in Python. Adding a university means editing `admissions_data.py` and redeploying.
- No user accounts. Chat history resets on page refresh. Checklist state is browser-local â€” different device means starting over.
- Eight programs only. More can be added but require code changes.
- Data is from one point in time. Deadlines, fees, and essay prompts change every application cycle. **Always verify on the official university website before actually applying.**
- Chat sidebar is hidden on mobile â€” program context selection isn't accessible on small screens.
- Token counts use the 4-characters-per-token approximation, not exact model tokenization.
- No way to save or revisit a previous chat session.

---

## ğŸ’¡ What I Learned

Going in I had basic Python knowledge and hadn't built anything with an LLM API before.

**The context injection pattern** was the biggest thing that clicked. Fine-tuning a model to know specific admissions requirements would be expensive, slow, and brittle every time requirements change. Injecting the real data into the system prompt at request time is free, instant, and always current when you update the dict. That's how most practical LLM applications actually work â€” and building it made it stick in a way that reading about it didn't.

**The debugging was real learning too.** When every AI response was a fallback string, I had no idea where to start. Going through the call stack and finding the silent import failure taught me to never trust a generic fallback â€” surface every error specifically, because "something went wrong" tells you nothing at 11pm when the thing you built isn't working.

**Compression was a surprise.** I expected a small improvement. It made a more noticeable difference â€” tighter context means the model focuses on the relevant parts rather than parsing boilerplate. The stuff between fetching data and sending it to the model matters more than I assumed going in.

Had help from AI along the way for certain parts. But the learning was real â€” a lot of things clicked that just wouldn't have from watching tutorials. Building something real is just a different kind of learning. ğŸ› ï¸